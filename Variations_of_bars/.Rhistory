}
if(progress == TRUE){
setTxtProgressBar(sample_progress, i)
}
}
if(progress == TRUE){
writeLines("\n")
}
#cleaning up the matrices and counts
if(length(all_k_best) != 0){
colnames(all_k_best) = c(1:ncol(all_k_best))
}
final.propose = c(a.count, s.count, m.count, j.count)
final.accept = c(add.accept.count, sub.accept.count, move.accept.count, jiggle.accept.count)
colnames(all_BIC) = "BIC"
#cleaning up beta/sigma draws
if(fit_storage == TRUE){
colnames(all_MSE) = "MSE"
colnames(all_fits) = c(1:ncol(all_fits))
post_beta_list = post_beta_list[,-1]
post_sigma_list = post_sigma_list[,-1]
rownames(post_beta_list) = c(seq(0,ma,1))
rownames(post_beta_list) = paste("B", rownames(post_beta_list), sep = "")
rownames(post_sigma_list) = "Sigma"
split_num = NULL #initializing
for(i in 2:ncol(post_beta_list)){ #detecting where to split up columns in beta/sigma object
if(startsWith(colnames(post_beta_list)[i], "1.") == TRUE){
split_num = c(split_num, i)
}
}
final_beta_list = list() #initializing
for(i in 1:length(split_num)){ #splitting up columns in beta object
if(i == 1){ #betas from first run
final_beta_list[[i]] = post_beta_list[,1:(split_num[i]-1)]
if(is.atomic(final_beta_list[[i]]) == T){
final_beta_list[[i]] = as.data.frame(final_beta_list[[i]])
rownames(final_beta_list[[i]]) = rownames(post_beta_list)
colnames(final_beta_list[[i]]) = 1
}else{
colnames(final_beta_list[[i]]) = c(1:ncol(final_beta_list[[i]]))
}
}else if(i < length(split_num)){# betas from middle runs
final_beta_list[[i]] = post_beta_list[,split_num[i-1]:(split_num[i]-1)]
if(is.atomic(final_beta_list[[i]]) == T){
final_beta_list[[i]] = as.data.frame(final_beta_list[[i]])
rownames(final_beta_list[[i]]) = rownames(post_beta_list)
colnames(final_beta_list[[i]]) = 1
}else{
colnames(final_beta_list[[i]]) = c(1:ncol(final_beta_list[[i]]))
}
}else{ #betas from penultimate and final runs
final_beta_list[[i]] = post_beta_list[,split_num[i-1]:(split_num[i]-1)]
if(is.atomic(final_beta_list[[i]]) == T){
final_beta_list[[i]] = as.data.frame(final_beta_list[[i]])
rownames(final_beta_list[[i]]) = rownames(post_beta_list)
colnames(final_beta_list[[i]]) = 1
}else{
colnames(final_beta_list[[i]]) = c(1:ncol(final_beta_list[[i]]))
}
final_beta_list[[i+1]] = post_beta_list[,split_num[i]:ncol(post_beta_list)]
if(is.atomic(final_beta_list[[i+1]]) == T){
final_beta_list[[i+1]] = as.data.frame(final_beta_list[[i+1]])
rownames(final_beta_list[[i+1]]) = rownames(post_beta_list)
colnames(final_beta_list[[i+1]]) = 1
}else{
colnames(final_beta_list[[i+1]]) = c(1:ncol(final_beta_list[[i+1]]))
}
}
}
post_beta_list = final_beta_list #saving final version of beta object
final_sigma_list = list() #initializing
for(i in 1:length(split_num)){ #splitting up columns in sigma object
if(i == 1){ #sigmas from first run
final_sigma_list[[i]] = post_sigma_list[,1:(split_num[i]-1)]
if(is.atomic(final_sigma_list[[i]]) == T){
final_sigma_list[[i]] = as.data.frame(final_sigma_list[[i]])
rownames(final_sigma_list[[i]]) = rownames(post_sigma_list)
colnames(final_sigma_list[[i]]) = 1
}else{
colnames(final_sigma_list[[i]]) = c(1:ncol(final_sigma_list[[i]]))
}
}else if(i < length(split_num)){# sigmas from middle runs
final_sigma_list[[i]] = post_sigma_list[,split_num[i-1]:(split_num[i]-1)]
if(is.atomic(final_sigma_list[[i]]) == T){
final_sigma_list[[i]] = as.data.frame(final_sigma_list[[i]])
rownames(final_sigma_list[[i]]) = rownames(post_sigma_list)
colnames(final_sigma_list[[i]]) = 1
}else{
colnames(final_sigma_list[[i]]) = c(1:ncol(final_sigma_list[[i]]))
}
}else{ #sigma from penultimate and final runs
final_sigma_list[[i]] = post_sigma_list[,split_num[i-1]:(split_num[i]-1)]
if(is.atomic(final_sigma_list[[i]]) == T){
final_sigma_list[[i]] = as.data.frame(final_sigma_list[[i]])
rownames(final_sigma_list[[i]]) = rownames(post_sigma_list)
colnames(final_sigma_list[[i]]) = 1
}else{
colnames(final_sigma_list[[i]]) = c(1:ncol(final_sigma_list[[i]]))
}
final_sigma_list[[i+1]] = post_sigma_list[,split_num[i]:ncol(post_sigma_list)]
if(is.atomic(final_sigma_list[[i+1]]) == T){
final_sigma_list[[i+1]] = as.data.frame(final_sigma_list[[i+1]])
rownames(final_sigma_list[[i+1]]) = rownames(post_sigma_list)
colnames(final_sigma_list[[i+1]]) = 1
}else{
colnames(final_sigma_list[[i+1]]) = c(1:ncol(final_sigma_list[[i+1]]))
}
}
}
post_sigma_list = final_sigma_list #saving final version of sigma object
}
#getting distribution of k (number of breakpoints)
num_bkpts = list()
for(i in 1:iterations){
current_k = length(all_k_best[i,][!is.na(all_k_best[i,])])
num_bkpts = c(num_bkpts, current_k, recursive=T)
}
if(fit_storage == TRUE){
final_list = list(accept_count / iterations, final.propose, final.accept, all_MSE, all_BIC, all_k_best, num_bkpts, post_beta_list, post_sigma_list, all_fits)
names(final_list) = c("AcceptRate", "ProposedSteps", "AcceptedSteps", "MSE", "BIC", "Breakpoints", "NumBkpts", "Beta", "Sigma", "Fits")
}else{
final_list = list(accept_count / iterations, final.propose, final.accept, all_BIC, all_k_best, num_bkpts)
names(final_list) = c("AcceptRate", "ProposedSteps", "AcceptedSteps", "BIC", "Breakpoints", "NumBkpts")
}
return(final_list)
}
#calling the function
#test_data = test_data_44()
#current_result = baar(NA, test_data[,1], test_data[,2], 50, 50, jump=0.25, ma=1, progress=T, fit_storage=T)
source("C:/Users/mellm/github/REU2022/Variations_of_bars/fixedbaar1.r")
source("C:/Users/mellm/github/REU2022/Variations_of_bars/fixedbaar1.r")
source("C:/Users/mellm/github/REU2022/Variations_of_bars/fixedbaar1.r")
View(baar)
view(baar)
View(baar)
library(MASS, lib.loc = "C:/Program Files/R/R-4.2.0/library")
install.packages("forecast")
library(forecast)
model
source("C:/Users/mellm/github/REU2022/Variations_of_bars/bama1.r")
model
bama.model
bama(model)
library(MASS, lib.loc = "C:/Program Files/R/R-4.2.0/library")
Qinv <- chol2inv(chol(Q))
tmp <- drop(y %*% Qinv %*% cbind(y)) / n
Sval <- tmp * (det(Q))^(1/n)
fitMetrics<-function(k_ends, full_data){
#create sum objects
sum_loglik = 0
coef_1 = 0
coef_2 = 0
#get and sum log likelihood for regressions of all intervals
if(length(k_ends) < 3 ){
model = suppressWarnings(arima(full_data[,2], order=c(0,0,ma)))
SEE = sum(na.omit(model$res)^2)
s2 = SEE/n
sum_loglik = (-1*n/2)*(log(2*pi)+log(s2)+1) #finding the log likelihoods on the full dataset
}else{
for(i in 2:length(k_ends)) {
if(i == 2){
min = k_ends[i-1]
y_values = full_data[c(min:k_ends[i]),2] #getting the y values in the interval
model = suppressWarnings(arima(y_values, c(0,0,ma)))
sub_n = length(y_values)
SEE = sum(na.omit(model$res)^2)
s2 = SEE/sub_n
sub_loglik = (-1*sub_n/2)*(log(2*pi)+log(s2)+1)
sum_loglik = sum_loglik + sub_loglik #the logLik looks the log likelyhood (relates to both SSR and MLE)
}else if(i > 2){
min = k_ends[i-1]
y_values = full_data[c((min+1):k_ends[i]),2] #getting the y values in the interval
model = suppressWarnings(arima(y_values, c(0,0,ma)))
sub_n = length(y_values)
SEE = sum(na.omit(model$res)^2)
s2 = SEE/sub_n
sub_loglik = (-1*sub_n/2)*(log(2*pi)+log(s2)+1)
sum_loglik = sum_loglik + sub_loglik #the logLik looks the log likelihood (relates to both SSR and MLE)
}
}
}
return(sum_loglik)
}
model
#Complete BAMA (Bayesian Adaptive Moving-Average)
#-------Key:
# k			= x-axis values of starting breakpoints
# time		= integer x-values of the entire data set
# data		= y-values of entire data set
# interations	= number of runs for final sampling with Metropolis-Hastings
# burn_in		= number of runs for set-up sampling with Metropolis-Hastings
# make_murder_p	= th e combine proportion (decimal) for make and murder steps
# note: move proportion is 1 - make_murder_p
# percent		= how much a point can jiggle
# lambda		= for Poisson distribution of breakpoint prior
# jump_p		= proportion of move steps that will be jump
# note: jiggle proprtion is 1 - jump_p
# ma			= order of MA model
# progress		= whether to show progress bars or not, TRUE/FALSE
# fit_storage	= whether or not to store betas, sigmas, and fits for each iteration, TRUE/FALSE
bama = function(k, time, data, iterations, burn_in = 50, make_murder_p = 0.5, percent = 0.02, lambda = 1, jump_p = 0.25, ma = 1, progress = TRUE, fit_storage = TRUE){
ma = floor(ma)
if(length(time) != length(data)){
return("Data and time vectors must be of equal length.")
}else if(length(data) < (6 * ma)){
return("Data insufficient for order of AR model. Try a lower order.")
}else if(make_murder_p >= 1){
return("Make/murder proportion must be less than 1.")
}else if(percent >= 0.5){
return("Percent for jiggle neighrborhood must be less than 0.5.")
}else if(jump_p > 1){
return("Jump proportion must be less than or equal to 1.")
}
library(MASS)
#library(FitAR)
full_data = cbind(c(1:length(as.numeric(time))), as.numeric(data)) #combining time and data inputs
n = length(full_data[,1]) #number of observations
k_ends <<- c(min(full_data[,1]), na.omit(k), n) #adding end points to k
#function to get sum of log likelihoods
fitMetrics<-function(k_ends, full_data){
par(mar=c(1,1,1,1))
setwd("/Users/mellm/github/REU2022/test_Cases")
pelican = read.csv("pacificBrownPelican.csv")
data.ts <- as.ts(pelican$Count_yr)
model = arima(data.ts, order=c(0,0,ma)) #this works, need to extract LL and sum
loglikelihood = logLik(model)
source("~/bama1.r")
source("~/bama1.r")
source("~/bama1.r")
source("~/bama1.r")
source("~/bama1.r")
source("~/bama1.r")
source("~/bama1.r")
y_values = full_data[c(min:k_ends[i]),2] #getting the y values in the interval
source("C:/Users/mellm/github/REU2022/Variations_of_bars/fixedbaar1.r")
model = suppressWarnings(FitAR(y_values, p=ar))
# install.packages("devtools")
# library(devtools)
# install_version("FitAR", version = "1.94", repos = "http://cran.us.r-project.org")
library(FitAR)
# install.packages("devtools")
# library(devtools)
# install_version("FitAR", version = "1.94", repos = "http://cran.us.r-project.org")
library(FitAR)
install.packages("devtools")
library(devtools)
library(devtools)
library(devtools)
install.packages("devtools")
library(devtools)
install_version("FitAR", version = "1.94", repos = "http://cran.us.r-project.org")
install.packages("devtools")
library(devtools)
install_version("FitAR", version = "1.94", repos = "http://cran.us.r-project.org")
library(FitAR)
source("C:/Users/mellm/github/REU2022/baar_simulations.R")
#D_n sum
k = 0:2l; (sum(big_I - big_theta(big_B))^k)x_t
source("C:/Users/mellm/github/REU2022/Variations_of_bars/fixedbaar1.r")
beta = -1
sd = 8
while (beta <= 1){
while (sd > 0){
y[i] = beta*y[i-1]+rnorm(1,0,sd)
plot(y)
sd = sd - 1
beta = beta + 0.1
}
}
q = 5 # a positive integer
l = function(q){
l = 1
while(2*l < q){
l = l + 1
}
return(l)
l
l(q)
l = function(q){
l = 1
while(2*l < q){
l = l + 1
}
return(l)
}
l(q)
l
z = runif(1,0,1) #1 value between 0 and 1
n = 25
hat_theta = theta_new #TEST
t = seq(1:10) #sequence of integers
q = 5 # a positive integer
Z_t = abs(rnorm(t)) #iid sequence of non-negative random variables
l = function(q){ #first integer such that 2*l >= q
l = 1
while(2*l < q){
l = l + 1
}
return(l)
}
l(q)
l = l(q)
#D_n t
D_nt = seq(2*l*q + 1,n)
D_nt = seq(2*l*q + 1,41)
D_nt = seq(2*l*q + 1,n)
n = 40
D_nt = seq(2*l*q + 1,n)
big_B = function(process){ #NEEDS WORK
for (t in length(process)){
big_B[t] = (process[t])/(process[t+1])
}
return(big_B)
}
big_B = function(process){ #NEEDS WORK
for (t in length(process)){
big_B[t] = (process[t])/(process[t+1])
}
return(big_B)
}
big_B(Z_t)
big_B = function(process){ #NEEDS WORK
for (i in t){
big_B[t] = (process[t])/(process[t+1])
}
return(big_B)
}
big_B(Z_t)
big_B = function(process){ #NEEDS WORK
for (i in t){
big_B[t] = (process[i])/(process[i+1])
}
return(big_B)
}
big_B(Z_t)
typeof(Z_t)
typeof(big_B)
big_B = function(process){ #NEEDS WORK
big_B = c()
for (i in t){
big_B[t] = (process[i])/(process[i+1])
}
return(big_B)
}
big_B(Z_t)
big_B = function(process){ #NEEDS WORK
big_B = c()
for (i in length(process)){
big_B[t] = (process[i])/(process[i+1])
}
return(big_B)
}
big_B(Z_t)
big_B(Z_t)
Z_t
Z_t[1]
Z_t[1]/Z_t[2]
big_B = function(process){ #NEEDS WORK
big_B = c()
for (i in length(process)){
big_B[i] = (process[i])/(process[i+1])
}
return(big_B)
}
big_B(Z_t)
big_B(Z_t)
starting_bkpts
starting_bkpts = 3
k_ends_new = 4
lambda = 5
q2 = 6
old_new_product_birth = function(product_runs){
for (j in k){
product_runs = append(product_runs,(n-(3*ar-(q1+1)*(2*ar + j))))
}
return(prod(product_runs))
starting_bkpts = 3
k_ends_new = 4
lambda = 5
q2 = 6
prior = c()
birth_old_to_new_ratio = (factorial(starting_bkpts+1)*q2*(dpois(k_ends_new,lambda)))/(old_new_product_birth(starting_bkpts))
starting_bkpts = 3
k_ends_new = 4
lambda = 5
q2 = 6
prior = c()
birth_old_to_new_ratio = (factorial(starting_bkpts+1)*q2*(dpois(k_ends_new,lambda)))/(old_new_product_birth(starting_bkpts))
starting_bkpts = 3
k_ends_new = 4
lambda = 5
q2 = 6
prior = c()
birth_old_to_new_ratio = (factorial(starting_bkpts+1)*q2*(dpois(k_ends_new,lambda)))/(old_new_product_birth(starting_bkpts))
big_I = x_t(B_theta,B_B,Z_t,q)
setwd("\\Users\\mellm\\github\\REU2022")
source("loglikelihoodma.r")
z_0 = runif(1,0,1) #1 value between 0 and 1
n = 40
hat_theta = theta_new #TEST
t = seq(1:10) #sequence of integers
q = 5 # a positive integer
Z_t = c(abs(rnorm(length(t-1),1))) #iid sequence of non-negative random variables
#x_sum
i = 1:q; x_sum = sum(hat_theta[i]*Z_t[i-1])
#MA equation (work in progress)
x_t = function (B_theta,B_B,Z_t,q){
B_theta * B_B[1:q]*Z_t[1:q]
}
big_theta = function(z_0){
i = 0:q; big_theta = sum(hat_theta[i]*z_0^i)
return(big_theta) #assumed to be non-zero
}
B_theta = big_theta(z_0)
l = function(q){ #first integer such that 2*l >= q
l = 1
while(2*l < q){
l = l + 1
}
return(l)
}
l = l(q)
big_B = function(process){ #NEEDS WORK
b_B = c()
for (i in t){
b_B[i] = (process[i])/(process[i+1])
}
return(b_B)
}
B_B = big_B(Z_t)
big_I = x_t(B_theta,B_B,Z_t,q)
#theta sum
i = 0:q; theta_sum = sum(hat_theta[i])
mean(L)
# download RTools if issues
# https://cran.r-project.org/bin/windows/Rtools/rtools42/rtools.html
# remember to close out of R after Rtools is downloaded
#install.packages("devtools")
library(devtools)
#install_version("FitAR", version = "1.94", repos = "http://cran.us.r-project.org")
library(FitAR)
library(strucchange)
library(MASS)
par(mfrow=c(2,2))
setwd("\\Users\\mellm\\github\\REU2022\\Variations_of_bars")
source("fixedbaar1.r")
test_data_45 = function(){
beta1 = .7
beta2 = .3
stdev= .25
y= rnorm(1,0,1)
for( i in 2:10){
y[i] = beta1*y[i-1]+rnorm(1,0,stdev)}
for(i in 11:90){
y[i] = beta2*y[i-1]+rnorm(1,0,stdev)}
data_45=y
time = c(1:90)
test_data_45 = data.frame(time, data_45)
return(test_data_45)
}
iterations=250
runs=2
L=matrix(NA,nrow=iterations,ncol=runs)
M=NA
for(i in 1:runs){
y=test_data_45()
current_data = y
break_p = breakpoints(current_data[,2] ~ current_data[,1], breaks = 5, h = 0.1)
starting_breakpoints = break_p$breakpoints
test1=baar(starting_breakpoints,y[,1],y[,2],iterations)
print(i)
L[,i]=test1$NumBkpts
M[i]=length(starting_breakpoints[!is.na(starting_breakpoints)])
}
mean(L)
sd(L)
mean(M)
sd(M)
i = 1:q; x_sum = sum(hat_theta[i]*Z_t[i-1])
big_B = function(process){ #NEEDS WORK
b_B = c()
for (i in t){
b_B[i] = (process[i])/(process[i+1])
}
return(b_B)
}
big_I = x_t(B_theta,B_B,Z_t,q)
#D_n t
D_nt = seq(2*l*q + 1,n)
#argmax of D_n
hat_theta = list(hat_theta, theta)
hat_theta = list(hat_theta, theta)
outputs = sapply(hat_theta, D_n)
bestD_n = hat_theta[which.max(outputs)]
#argmax of D_n
hat_theta = list(hat_theta, 1)
#D_n sum
i = 0:(2*l); D_sum = (sum(big_I - big_theta(B_B))^i)*x_t
i = 0:(2*l); D_sum = (sum((big_I[i] - B_theta*B_B)^i))*x_t
i = 0:(2*l)
big_I[i]
#x_sum
i = 1:q; x_sum = sum(hat_theta[i]*Z_t[i-1])
at_theta
hat_theta
theta_new
theta_new[1]
Z_t[3]
#x_sum
i = 1:q; x_sum = sum(hat_theta[i]*Z_t[i-1])
#x_sum
i = 2:q; x_sum = sum(hat_theta[i]*Z_t[i-1])
